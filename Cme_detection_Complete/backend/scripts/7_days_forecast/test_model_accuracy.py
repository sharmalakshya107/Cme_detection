"""
Test Model Accuracy
==================
Tests the LSTM model accuracy against test data and displays comprehensive metrics.
"""
import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path

# Add parent directory to path
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from forecast_model_runner import ForecastModelRunner
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def test_model_accuracy():
    """Test model accuracy using test data"""
    print("="*70)
    print("MODEL ACCURACY TEST")
    print("="*70)
    
    # Load test data
    test_data_path = script_dir / 'test_data_unscaled.csv'
    if not test_data_path.exists():
        print(f"‚ùå Test data not found: {test_data_path}")
        print("   Using training data for validation instead...")
        test_data_path = script_dir / 'train_data_unscaled.csv'
        if not test_data_path.exists():
            print(f"‚ùå Training data also not found!")
            return None
    
    print(f"‚úì Loading test data from: {test_data_path}")
    test_data = pd.read_csv(test_data_path)
    
    # Get target columns
    target_vars = ['Dst_Index_nT', 'Kp_10', 'ap_index_nT']
    
    # Check if we have actual vs predicted data
    # For now, we'll test with recent predictions
    print("\n" + "="*70)
    print("TESTING MODEL PREDICTIONS")
    print("="*70)
    
    # Initialize model runner
    runner = ForecastModelRunner()
    
    try:
        # Make predictions
        print("\n1. Loading model...")
        runner.load_model()
        print("   ‚úì Model loaded")
        
        print("\n2. Making predictions...")
        predictions_df = runner.make_predictions()
        print(f"   ‚úì Generated {len(predictions_df)} predictions")
        
        # Get actual values from test data (if available)
        print("\n3. Comparing with test data...")
        
        # For forecast model, we can't directly compare with future values
        # But we can check:
        # 1. Prediction ranges are reasonable
        # 2. Model structure is correct
        # 3. Values are in expected ranges
        
        accuracy_report = {
            'model_info': {
                'model_path': str(runner.model_path),
                'input_shape': runner.model.input_shape if runner.model else None,
                'output_shape': runner.model.output_shape if runner.model else None,
                'target_variables': runner.target_vars
            },
            'prediction_statistics': {},
            'value_range_validation': {},
            'expected_ranges': {
                'Dst_Index_nT': {'min': -200, 'max': 50, 'typical': (-100, 20)},
                'Kp_10': {'min': 0, 'max': 9, 'typical': (0, 9)},
                'ap_index_nT': {'min': 0, 'max': 400, 'typical': (0, 400)},
                'Sunspot_Number': {'min': 0, 'max': 300, 'typical': (0, 200)}
            }
        }
        
        # Check each target variable
        for target in runner.target_vars:
            if target in predictions_df.columns:
                values = predictions_df[target].values
                
                # Statistics
                accuracy_report['prediction_statistics'][target] = {
                    'count': len(values),
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'median': float(np.median(values))
                }
                
                # Range validation
                expected = accuracy_report['expected_ranges'].get(target, {})
                min_val = expected.get('min', -np.inf)
                max_val = expected.get('max', np.inf)
                
                in_range = np.sum((values >= min_val) & (values <= max_val))
                range_percentage = (in_range / len(values)) * 100
                
                accuracy_report['value_range_validation'][target] = {
                    'expected_range': f"{min_val} to {max_val}",
                    'actual_range': f"{np.min(values):.2f} to {np.max(values):.2f}",
                    'values_in_range': int(in_range),
                    'percentage_in_range': float(range_percentage),
                    'is_valid': range_percentage > 95.0
                }
        
        # Display results
        print("\n" + "="*70)
        print("ACCURACY REPORT")
        print("="*70)
        
        print(f"\nüìä Model Information:")
        print(f"   Model: {Path(runner.model_path).name}")
        print(f"   Input Shape: {accuracy_report['model_info']['input_shape']}")
        print(f"   Output Shape: {accuracy_report['model_info']['output_shape']}")
        print(f"   Targets: {', '.join(accuracy_report['model_info']['target_variables'])}")
        
        print(f"\nüìà Prediction Statistics:")
        for target, stats in accuracy_report['prediction_statistics'].items():
            print(f"\n   {target}:")
            print(f"      Count: {stats['count']}")
            print(f"      Mean: {stats['mean']:.4f}")
            print(f"      Std: {stats['std']:.4f}")
            print(f"      Min: {stats['min']:.4f}")
            print(f"      Max: {stats['max']:.4f}")
            print(f"      Median: {stats['median']:.4f}")
        
        print(f"\n‚úÖ Value Range Validation:")
        all_valid = True
        for target, validation in accuracy_report['value_range_validation'].items():
            status = "‚úì" if validation['is_valid'] else "‚úó"
            print(f"\n   {status} {target}:")
            print(f"      Expected: {validation['expected_range']}")
            print(f"      Actual: {validation['actual_range']}")
            print(f"      In Range: {validation['values_in_range']}/{len(predictions_df)} ({validation['percentage_in_range']:.1f}%)")
            if not validation['is_valid']:
                all_valid = False
        
        print(f"\n" + "="*70)
        if all_valid:
            print("‚úÖ MODEL ACCURACY: All predictions are within expected ranges!")
        else:
            print("‚ö†Ô∏è  WARNING: Some predictions are outside expected ranges")
        print("="*70)
        
        # Historical accuracy from training (from MODEL_OUTPUTS_AND_ACCURACY.md)
        print(f"\nüìö Historical Training Accuracy (from model training):")
        print(f"   Dst_Index_nT: MAE = 0.53 nT (excellent, range -200 to +50)")
        print(f"   Kp_10: MAE = 0.70 (good, range 0-9)")
        print(f"   ap_index_nT: MAE = 0.40 nT (excellent, range 0-400)")
        print(f"   Test Set: 19,213 sequences")
        print(f"   Training Set: 77,569 sequences")
        
        return accuracy_report
        
    except Exception as e:
        print(f"\n‚ùå Error testing model: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    report = test_model_accuracy()
    if report:
        print("\n‚úì Accuracy test completed successfully!")





